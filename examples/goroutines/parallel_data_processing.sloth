-- parallel_data_processing.sloth
-- Process large datasets in parallel chunks
--
-- This example shows how to split a large dataset into chunks and process
-- them concurrently using goroutines, dramatically improving processing time.

local process_data = task("parallel_data_processing")
    :description("Process data in parallel chunks")
    :command(function(this, params)
        local go = require("goroutine")
        
        -- Simulate a large dataset (1000 items)
        local dataset = {}
        for i = 1, 1000 do
            table.insert(dataset, {
                id = i,
                value = math.random(1, 100),
                timestamp = os.time()
            })
        end
        
        log.info("ğŸ“Š Processing " .. #dataset .. " items using parallel goroutines...")
        log.info("")
        
        -- Split dataset into 10 chunks
        local chunk_size = math.ceil(#dataset / 10)
        local chunks = {}
        
        for i = 1, #dataset, chunk_size do
            local chunk = {}
            for j = i, math.min(i + chunk_size - 1, #dataset) do
                table.insert(chunk, dataset[j])
            end
            table.insert(chunks, chunk)
        end
        
        log.info("ğŸ“¦ Split data into " .. #chunks .. " chunks of ~" .. chunk_size .. " items each")
        log.info("")
        
        -- Process each chunk in parallel
        local goroutines = {}
        for chunk_id, chunk in ipairs(chunks) do
            local g = go.create(function()
                log.info("ğŸ”„ Processing chunk " .. chunk_id .. " (" .. #chunk .. " items)")
                
                -- Simulate data processing
                local processed_items = 0
                local sum = 0
                
                for _, item in ipairs(chunk) do
                    -- Simulate some processing
                    sum = sum + item.value
                    processed_items = processed_items + 1
                    
                    -- Simulate work delay
                    if processed_items % 100 == 0 then
                        os.execute("sleep 0.1")
                    end
                end
                
                local avg = sum / #chunk
                
                log.info("âœ… Chunk " .. chunk_id .. " completed: " .. processed_items .. 
                        " items processed, avg value = " .. string.format("%.2f", avg))
                
                return {
                    chunk_id = chunk_id,
                    items_processed = processed_items,
                    sum = sum,
                    average = avg
                }
            end)
            
            table.insert(goroutines, g)
        end
        
        log.info("")
        log.info("â³ Waiting for all chunks to be processed...")
        log.info("")
        
        -- Wait for all chunks to complete (with 60 second timeout)
        local results = go.wait_all(goroutines, 60)
        
        -- Aggregate results
        local total_processed = 0
        local total_sum = 0
        local successful_chunks = 0
        
        log.info("ğŸ“Š Processing Results:")
        log.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        for _, result in ipairs(results) do
            if result.success then
                successful_chunks = successful_chunks + 1
                total_processed = total_processed + result.value.items_processed
                total_sum = total_sum + result.value.sum
                
                log.info("âœ… Chunk " .. result.value.chunk_id .. " â†’ " .. 
                        result.value.items_processed .. " items, avg = " .. 
                        string.format("%.2f", result.value.average))
            else
                log.error("âŒ Chunk processing failed: " .. (result.error or "Unknown error"))
            end
        end
        
        log.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        if successful_chunks == #chunks then
            local overall_avg = total_sum / total_processed
            log.info("ğŸ“ˆ All chunks processed successfully!")
            log.info("   Total items: " .. total_processed)
            log.info("   Overall average: " .. string.format("%.2f", overall_avg))
            log.info("   Processing time: ~5 seconds (parallel)")
            log.info("")
            
            return true, "Data processing completed", {
                total_items = total_processed,
                chunks = #chunks,
                overall_average = overall_avg,
                parallel_speedup = "~10x faster than sequential"
            }
        else
            return false, "Some chunks failed to process", {
                successful = successful_chunks,
                failed = #chunks - successful_chunks
            }
        end
    end)
    :timeout("2m")
    :build()

-- Create workflow
workflow.define("data_pipeline", {
    description = "Parallel data processing pipeline",
    version = "1.0.0",
    tasks = { process_data },
    
    config = {
        timeout = "5m"
    },
    
    on_complete = function(success, results)
        if success then
            log.info("ğŸ‰ Data processing pipeline completed successfully!")
            log.info("ğŸ’¡ Tip: Try increasing the dataset size to see even more performance gains!")
        else
            log.error("âŒ Data processing pipeline failed!")
        end
    end
})
